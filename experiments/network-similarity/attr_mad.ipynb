{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a858997d-1eb2-4616-a26f-dcc2e2d83586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "from idsds.models.resnet import resnet18, resnet50, resnet101, resnet152, wide_resnet50_2\n",
    "from idsds.models.vgg import vgg16, vgg16_bn, vgg13, vgg19, vgg11\n",
    "from idsds.models.ViT.ViT_new import vit_base_patch16_224\n",
    "from idsds.models.ViT.ViT_LRP import vit_base_patch16_224 as vit_LRP\n",
    "from idsds.models.bagnets.pytorchnet import bagnet33\n",
    "from idsds.models.xdnns.xfixup_resnet import xfixup_resnet50, fixup_resnet50\n",
    "from idsds.models.xdnns.xvgg import xvgg16\n",
    "from idsds.models.bcos_v2.bcos_resnet import resnet50 as bcos_resnet50\n",
    "from idsds.models.bcos_v2.bcos_resnet import resnet18 as bcos_resnet18\n",
    "\n",
    "import utils\n",
    "\n",
    "original_models = \"/workspace/hd/original/\"\n",
    "tuned_models = \"/workspace/hd/tuned/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f57030-1810-43bc-bcfc-be3d5bc37a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_mad_attr(model1, model2, layer1, layer2, attr_fn, loader):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model1.to(device).eval()\n",
    "    model2.to(device).eval()\n",
    "\n",
    "    total_mad = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    cam1 = attr_fn(model1, target_layer=layer1)\n",
    "    cam2 = attr_fn(model2, target_layer=layer2)\n",
    "\n",
    "    for images, _ in loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            input_tensor = images[i].unsqueeze(0)\n",
    "    \n",
    "            output1 = model1(input_tensor)\n",
    "            pred_class1 = output1.argmax(dim=1)[0].item()\n",
    "            attribution_map1 = cam1(pred_class1, output1)[0]\n",
    "        \n",
    "            output2 = model2(input_tensor)\n",
    "            pred_class2 = output2.argmax(dim=1)[0].item()\n",
    "            attribution_map2 = cam2(pred_class2, output2)[0]\n",
    "        \n",
    "            if attribution_map1.ndim == 3 and attribution_map1.shape[0] == 1:\n",
    "                attribution_map1 = attribution_map1.squeeze(0)\n",
    "            if attribution_map2.ndim == 3 and attribution_map2.shape[0] == 1:\n",
    "                attribution_map2 = attribution_map2.squeeze(0)\n",
    "            \n",
    "            map1_resized = F.interpolate(attribution_map1.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze()\n",
    "            map2_resized = F.interpolate(attribution_map2.unsqueeze(0).unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze()\n",
    "        \n",
    "            total_mad += torch.abs(map1_resized - map2_resized).mean().item()\n",
    "        num_samples += images.size(0)\n",
    "        print(f\"{num_samples} {total_mad}\")\n",
    "\n",
    "    mad = total_mad / num_samples\n",
    "    print(f\"Mean Absolute Difference between attribution maps: {mad}\")\n",
    "\n",
    "test_loader = utils.get_loader(\"/workspace/hd/imagenet-mini/val\", batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433be707-0075-44ee-b8e9-750d8e095cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/src/experiments/network-similarity/utils.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n",
      "WARNING:root:no value was provided for `target_layer`, thus set to 'layer4'.\n",
      "WARNING:root:no value was provided for `target_layer`, thus set to 'layer4'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 22.87311269622296\n",
      "1000 44.498108664061874\n",
      "1500 68.4017807324417\n",
      "2000 95.65968740312383\n",
      "2500 125.03875130461529\n",
      "3000 154.36896573146805\n",
      "3500 186.20815494144335\n",
      "3923 211.47061005374417\n",
      "Mean Absolute Difference between attribution maps: 0.053905330118211615\n"
     ]
    }
   ],
   "source": [
    "from torchcam.methods import GradCAM\n",
    "\n",
    "resnet50_ood = resnet50(pretrained=True)\n",
    "resnet50_id = resnet50(pretrained=True)\n",
    "resnet50_id = utils.load_state_dict(\n",
    "    tuned_models + \"resnet50_imagenet1000_lr0.001_epochs30_step10_checkpoint_best.pth.tar\", \n",
    "    resnet50_id\n",
    ")\n",
    "compare_mad_attr(resnet50_ood, resnet50_id, GradCAM, test_loader)\n",
    "# 0.053905330118211615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f661eaf7-c241-445c-9cab-9f96b0191d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 19.695862133055925\n",
      "1000 39.15862245950848\n",
      "1500 59.35514400340617\n",
      "2000 82.85035688849166\n",
      "2500 104.98470893176273\n",
      "3000 131.4548487327993\n",
      "3500 156.28242710512131\n",
      "3923 174.397152420599\n",
      "Mean Absolute Difference between attribution maps: 0.04445504777481494\n"
     ]
    }
   ],
   "source": [
    "vgg16_ood = vgg16(pretrained=True)\n",
    "vgg16_id = vgg16(pretrained=True)\n",
    "vgg16_id = utils.load_state_dict(\n",
    "    tuned_models + \"/vgg16_imagenet1000_lr0.001_epochs30_step10_checkpoint_best.pth.tar\", \n",
    "    vgg16_id\n",
    ")\n",
    "compare_mad_attr(vgg16_ood, vgg16_id, \"features\", \"features\", GradCAM, test_loader)\n",
    "# 0.04445504777481494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa4655d-6bde-46c9-ae3f-928e0d512371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
